---
# While we would prefer to use the Ansible helm module, it's broken! :-(
# See https://github.com/ansible/ansible/pull/57897
# Unfortunately this will not be fixed until Ansible 2.10 which is not yet released.
# So for now we will run /usr/local/bin/helm commands directly...

- name: install gpu-operator helm repo
  command: /usr/local/bin/helm repo add nvidia "{{ gpu_operator_helm_repo }}"

- name: update helm repos
  command: /usr/local/bin/helm repo update

# TODO: If this already exists we fail, so skip that for now with failed_when: false
- name: Create namespace for GPU Operator resources
  command: kubectl create namespace {{ gpu_operator_namespace }}
  when: gpu_operator_create_secret
  failed_when: false

# TODO: If this already exists we fail, so skip that for now with failed_when: false
- name: Create a docker secret for GPU Operator containers
  command: kubectl create secret docker-registry {{ gpu_operator_registry_secret }} --docker-server="{{ gpu_operator_driver_registry }}" --docker-username={{ gpu_operator_registry_username }} --docker-password={{ gpu_operator_registry_password }} --docker-email={{ gpu_operator_registry_email }} -n {{ gpu_operator_namespace }}
  when: gpu_operator_create_secret
  failed_when: false

- name: Set secret var for helm command line if specified
  set_fact: 
    gpu_operator_registry_secret: --set driver.imagePullSecrets[0]="{{ gpu_operator_registry_secret }}"
  when: gpu_operator_registry_secret != ""

- name: Copy DCGM CSV config file to host
  copy:
    src: "{{ gpu_operator_dcgm_config_csv }}"
    dest: "/tmp/{{ gpu_operator_dcgm_config_csv }}"

# TODO: If this already exists we fail, so skip that for now with failed_when: false
- name: Create a configmap for DCGM CSV config file
  command: kubectl create configmap dcgm-custom-metrics --from-file="/tmp/{{ gpu_operator_dcgm_config_csv }}" -n {{ gpu_operator_namespace }}

- name: Copy values.yml to host
  template:
    src: "{{ gpu_operator_values_file }}"
    dest: "/tmp/{{ gpu_operator_values_file }}"

# XXX: This currently installs into the default namespace, as per the GPU Operator docs
# https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html
- name: install nvidia gpu operator
  command: /usr/local/bin/helm upgrade --install "{{ gpu_operator_release_name }}" "{{ gpu_operator_chart_name }}" --version "{{ gpu_operator_chart_version }}" -f "/tmp/{{ gpu_operator_values_file }}"
